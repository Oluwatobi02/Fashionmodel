{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Oluwatobi02/Fashionmodel/blob/main/FashionModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLboQUZHOYrL",
    "outputId": "9ee6e7b0-9222-44ef-9e9b-6d56dd7e190f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./venv/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: transformers in ./venv/lib/python3.12/site-packages (4.47.0)\n",
      "Requirement already satisfied: pillow in ./venv/lib/python3.12/site-packages (11.0.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./venv/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./venv/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./venv/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./venv/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./venv/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./venv/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./venv/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./venv/lib/python3.12/site-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.12/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "RhUhGzhzOco_"
   },
   "outputs": [],
   "source": [
    "from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation, ViltProcessor, ViltForQuestionAnswering\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "ciRA2nDNOgqi"
   },
   "outputs": [],
   "source": [
    "processor = SegformerImageProcessor.from_pretrained(\"sayeed99/segformer-b3-fashion\")\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(\"sayeed99/segformer-b3-fashion\")\n",
    "processor_nlp = ViltProcessor.from_pretrained(\"yanka9/vilt_finetuned_deepfashionVQA_v2\")\n",
    "model_nlp = ViltForQuestionAnswering.from_pretrained(\"yanka9/vilt_finetuned_deepfashionVQA_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "7SC7tTgGQpmo"
   },
   "outputs": [],
   "source": [
    "categories = model.config.id2label\n",
    "del categories[0]\n",
    "categories[1] = \"shirt or blouse\"\n",
    "categories[2] = \"top, t-shirt or sweatshirt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_cat = [\"color\", \"style\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_columns():\n",
    "    columns = []\n",
    "    for idx in range(1, 9):\n",
    "        if idx in [52,53]:\n",
    "            continue\n",
    "        it_col = []\n",
    "        it_col.append(f\"item {idx} type\")\n",
    "        for i in other_cat:\n",
    "            it_col.append(f\"item {idx} {i}\")\n",
    "        columns.extend(it_col)\n",
    "    return columns\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "fash_col = create_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset.csv\", \"a\") as file:\n",
    "    file.write(\",\".join(fash_col)+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(path, item):\n",
    "    image = Image.open(path)\n",
    "    answer = []\n",
    "    questions = [f\"what is the color of the {item}?\", f\"is this {item} casual or formal\"]\n",
    "\n",
    "    for text in questions:\n",
    "\n",
    "    # prepare inputs\n",
    "        encoding = processor_nlp(image, text, return_tensors=\"pt\")\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model_nlp(**encoding)\n",
    "        logits = outputs.logits\n",
    "        idx = logits.argmax(-1).item()\n",
    "        answer.append(model_nlp.config.id2label[idx])\n",
    "    return tuple(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(path, items):\n",
    "    image = Image.open(path)\n",
    "    text = f\"{items}?\"\n",
    "\n",
    "    # prepare inputs\n",
    "    encoding = processor_nlp(image, text, return_tensors=\"pt\")\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model_nlp(**encoding)\n",
    "    logits = outputs.logits\n",
    "    idx = logits.argmax(-1).item()\n",
    "    return model_nlp.config.id2label[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "takeouts = [\"glasses\", \"collar\", \"sleeve\", \"buckle\", \"belt\", ]\n",
    "\n",
    "def get_categories(path):\n",
    "    image = Image.open(path)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits.cpu()\n",
    "    unsampled_logits = nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False\n",
    "    )\n",
    "    pred_seg = unsampled_logits.argmax(dim=1)[0]\n",
    "    unique_cat = np.unique(pred_seg.cpu().numpy())\n",
    "    result = []\n",
    "    for group_no in unique_cat:\n",
    "        if group_no in [1,2]:\n",
    "            group = categories[group_no]\n",
    "            result.append(ask_question(path, group))\n",
    "        elif group_no == 0: continue\n",
    "        else:\n",
    "            result.append(categories[group_no])\n",
    "    for i in takeouts:\n",
    "        if i in result:\n",
    "            result.remove(i)\n",
    "    return result\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(row, csv_file=\"dataset.csv\"):\n",
    "    with open(csv_file, \"a\") as file:\n",
    "        file.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def handle_image(path):\n",
    "    labels = get_categories(path)\n",
    "    result = []\n",
    "    for label in labels:\n",
    "        result.append(label)\n",
    "        metadata = get_metadata(path, label)\n",
    "        result.extend(metadata)\n",
    "    t1 = threading.Thread(target=write_to_csv, args=(\",\".join(result),))\n",
    "    t1.start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\"1.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"running main...\")\n",
    "    for image_path in images:\n",
    "        p1 = threading.Thread(target=handle_image, args=(f\"./data/{image_path}\",))\n",
    "        p1.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running main...\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMllrWXATUTIfh55586QJGH",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
